{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: monai in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: nibabel in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (5.3.2)\n",
      "Requirement already satisfied: scikit-learn in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: networkx in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from monai) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20 in /usr/lib/python3/dist-packages (from nibabel) (21.3)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from nibabel) (6.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/UWO/msnyde26/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch monai nibabel scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19feac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Rand3DElasticd,\n",
    "    ScaleIntensityd,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    RandFlipd,\n",
    "    ToTensord,\n",
    "    GridPatchd,\n",
    "    MapTransform\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "import torch.nn as nn\n",
    "import time \n",
    "\n",
    "from monai.utils import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.inferers import sliding_window_inference\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f18c9",
   "metadata": {},
   "source": [
    "changable model params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "patch_size = 64\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "filter_num_G = 32\n",
    "filter_num_D = 32\n",
    "depth_G = 4\n",
    "train_steps_d = 3\n",
    "loss_func = \"mae\"\n",
    "\n",
    "# patch_size = (16, 32)\n",
    "# batch_size = (32, 64, 128)\n",
    "# lr = (0.0001, 0.001, 0.01)\n",
    "# filter_num_G = (16, 32, 64)\n",
    "# filter_num_D = (16, 32, 64)\n",
    "# depth_G = (3, 4)\n",
    "# train_steps_d = (3,4)\n",
    "# loss_func = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224584be",
   "metadata": {},
   "source": [
    "define model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e00d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"output/image_size-{image_size}_patch_size-{patch_size}_batch-{batch_size}_LR-{lr}_filter_G-{filter_num_G}_filter_D-{filter_num_D}_depth_G-{depth_G}_train_steps_d_{train_steps_d}_loss_func_{loss_func}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c07a6",
   "metadata": {},
   "source": [
    "define available resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "191b7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available \n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08334c",
   "metadata": {},
   "source": [
    "organize data dict for model inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1746432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P004/ses-pre/normalize/sub-P004_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P004/ses-pre/normalize/sub-P004_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P005/ses-pre/normalize/sub-P005_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P005/ses-pre/normalize/sub-P005_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P006/ses-pre/normalize/sub-P006_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P006/ses-pre/normalize/sub-P006_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P007/ses-pre/normalize/sub-P007_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P007/ses-pre/normalize/sub-P007_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P008/ses-pre/normalize/sub-P008_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P008/ses-pre/normalize/sub-P008_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P010/ses-pre/normalize/sub-P010_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P010/ses-pre/normalize/sub-P010_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P011/ses-pre/normalize/sub-P011_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P011/ses-pre/normalize/sub-P011_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P012/ses-pre/normalize/sub-P012_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P012/ses-pre/normalize/sub-P012_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P014/ses-pre/normalize/sub-P014_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P014/ses-pre/normalize/sub-P014_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P015/ses-pre/normalize/sub-P015_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P015/ses-pre/normalize/sub-P015_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P016/ses-pre/normalize/sub-P016_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P016/ses-pre/normalize/sub-P016_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P017/ses-pre/normalize/sub-P017_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P017/ses-pre/normalize/sub-P017_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P018/ses-pre/normalize/sub-P018_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P018/ses-pre/normalize/sub-P018_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P019/ses-pre/normalize/sub-P019_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P019/ses-pre/normalize/sub-P019_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P020/ses-pre/normalize/sub-P020_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P020/ses-pre/normalize/sub-P020_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P021/ses-pre/normalize/sub-P021_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P021/ses-pre/normalize/sub-P021_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P022/ses-pre/normalize/sub-P022_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P022/ses-pre/normalize/sub-P022_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P023/ses-pre/normalize/sub-P023_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P023/ses-pre/normalize/sub-P023_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P024/ses-pre/normalize/sub-P024_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P024/ses-pre/normalize/sub-P024_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P025/ses-pre/normalize/sub-P025_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P025/ses-pre/normalize/sub-P025_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P026/ses-pre/normalize/sub-P026_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P026/ses-pre/normalize/sub-P026_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P028/ses-pre/normalize/sub-P028_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P028/ses-pre/normalize/sub-P028_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P029/ses-pre/normalize/sub-P029_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P029/ses-pre/normalize/sub-P029_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P030/ses-pre/normalize/sub-P030_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P030/ses-pre/normalize/sub-P030_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P031/ses-pre/normalize/sub-P031_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P031/ses-pre/normalize/sub-P031_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P032/ses-pre/normalize/sub-P032_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P032/ses-pre/normalize/sub-P032_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P033/ses-pre/normalize/sub-P033_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P033/ses-pre/normalize/sub-P033_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P034/ses-pre/normalize/sub-P034_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P034/ses-pre/normalize/sub-P034_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P035/ses-pre/normalize/sub-P035_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P035/ses-pre/normalize/sub-P035_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P036/ses-pre/normalize/sub-P036_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P036/ses-pre/normalize/sub-P036_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P037/ses-pre/normalize/sub-P037_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P037/ses-pre/normalize/sub-P037_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P038/ses-pre/normalize/sub-P038_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P038/ses-pre/normalize/sub-P038_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P039/ses-pre/normalize/sub-P039_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P039/ses-pre/normalize/sub-P039_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P041/ses-pre/normalize/sub-P041_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P041/ses-pre/normalize/sub-P041_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P042/ses-pre/normalize/sub-P042_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P042/ses-pre/normalize/sub-P042_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P043/ses-pre/normalize/sub-P043_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P043/ses-pre/normalize/sub-P043_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P044/ses-pre/normalize/sub-P044_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P044/ses-pre/normalize/sub-P044_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P045/ses-pre/normalize/sub-P045_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P045/ses-pre/normalize/sub-P045_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P046/ses-pre/normalize/sub-P046_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P046/ses-pre/normalize/sub-P046_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P047/ses-pre/normalize/sub-P047_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P047/ses-pre/normalize/sub-P047_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P048/ses-pre/normalize/sub-P048_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P048/ses-pre/normalize/sub-P048_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P051/ses-pre/normalize/sub-P051_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P051/ses-pre/normalize/sub-P051_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P052/ses-pre/normalize/sub-P052_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P052/ses-pre/normalize/sub-P052_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P053/ses-pre/normalize/sub-P053_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P053/ses-pre/normalize/sub-P053_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P054/ses-pre/normalize/sub-P054_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P054/ses-pre/normalize/sub-P054_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P055/ses-pre/normalize/sub-P055_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P055/ses-pre/normalize/sub-P055_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P056/ses-pre/normalize/sub-P056_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P056/ses-pre/normalize/sub-P056_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P057/ses-pre/normalize/sub-P057_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P057/ses-pre/normalize/sub-P057_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P058/ses-pre/normalize/sub-P058_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P058/ses-pre/normalize/sub-P058_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P059/ses-pre/normalize/sub-P059_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P059/ses-pre/normalize/sub-P059_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P060/ses-pre/normalize/sub-P060_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P060/ses-pre/normalize/sub-P060_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P061/ses-pre/normalize/sub-P061_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P061/ses-pre/normalize/sub-P061_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P062/ses-pre/normalize/sub-P062_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P062/ses-pre/normalize/sub-P062_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P063/ses-pre/normalize/sub-P063_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P063/ses-pre/normalize/sub-P063_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P064/ses-pre/normalize/sub-P064_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P064/ses-pre/normalize/sub-P064_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P065/ses-pre/normalize/sub-P065_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P065/ses-pre/normalize/sub-P065_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P066/ses-pre/normalize/sub-P066_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P066/ses-pre/normalize/sub-P066_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P068/ses-pre/normalize/sub-P068_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P068/ses-pre/normalize/sub-P068_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P069/ses-pre/normalize/sub-P069_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P069/ses-pre/normalize/sub-P069_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Loaded 60 paired samples.\n"
     ]
    }
   ],
   "source": [
    "# creates a dictionary of pairs of image file paths (str)\n",
    "\n",
    "input_dir = os.path.expanduser(\"~/graham/scratch/degad_preprocessed_data\")\n",
    "\n",
    "work_dir = os.path.join(input_dir, \"work\")\n",
    "subject_dirs = glob.glob(os.path.join(work_dir, \"sub-*\"))\n",
    "\n",
    "subjects = []\n",
    "for directory in subject_dirs:\n",
    "    if os.path.isdir(directory): \n",
    "        subjects.append(directory)\n",
    "\n",
    "data_dicts = []\n",
    "for sub in subjects:   \n",
    "    gad_images = glob.glob(os.path.join(sub, \"ses-pre\", \"normalize\", \"*acq-gad*_T1w.nii.gz\"))\n",
    "    print(\"Found gad images:\", gad_images)\n",
    "    \n",
    "    nogad_images = glob.glob(os.path.join(sub, \"ses-pre\", \"normalize\", \"*acq-nongad*_T1w.nii.gz\"))\n",
    "    print(\"Found nogad images:\", nogad_images)\n",
    "    \n",
    "    if gad_images and nogad_images:\n",
    "        data_dicts.append({\"image\": gad_images[0], \"label\": nogad_images[0], \"image_filepath\": gad_images[0]})\n",
    "\n",
    "print(\"Loaded\", len(data_dicts), \"paired samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4f263",
   "metadata": {},
   "source": [
    "split into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cce638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 42, Val: 9, Test: 9\n"
     ]
    }
   ],
   "source": [
    "# create split of the image path strings\n",
    "\n",
    "# 85% train, 15% test \n",
    "\n",
    "train, test = train_test_split(data_dicts, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef55e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveImagePath(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Storing the file path separately in the 'image_filepath' key\n",
    "        data['image_filepath'] = data['image']\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceae697",
   "metadata": {},
   "source": [
    "Data transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims_tuple:  (256, 256, 256)\n",
      "Test image shape: torch.Size([1, 256, 256, 256])\n",
      "Test label shape: torch.Size([1, 256, 256, 256])\n",
      "Val image shape: torch.Size([1, 256, 256, 256])\n",
      "Val label shape: torch.Size([1, 256, 256, 256])\n",
      "Test image shape: torch.Size([1, 256, 256, 256])\n",
      "Test label shape: torch.Size([1, 256, 256, 256])\n",
      "Image file path: /home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# using transformations from original code \n",
    "\n",
    "# set size of image to patch size (patch_size, patch_size, patch_size)\n",
    "dims_tuple_image = (image_size,)*3\n",
    "dims_tuple_patches = (patch_size,)*3\n",
    "print(\"dims_tuple: \", dims_tuple_image)\n",
    "print(\"dims_tuple: \", dims_tuple_patches)\n",
    "\n",
    "# train tranforms \n",
    "train_transforms = Compose([\n",
    "    LoadImaged(\n",
    "        keys=[\"image\", \"label\"], \n",
    "    ),  # load image from the file path \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]), # ensure this is [C, H, W, (D)]\n",
    "    ScaleIntensityd(keys=[\"image\"]), # scales the intensity from 0-1\n",
    "    Rand3DElasticd(\n",
    "        keys = (\"image\",\"label\"), \n",
    "        sigma_range = (0.5,1), \n",
    "        magnitude_range = (0.1, 0.4), \n",
    "        prob=0.4, \n",
    "        shear_range=(0.1, -0.05, 0.0, 0.0, 0.0, 0.0),\n",
    "        scale_range=0.5, padding_mode= \"zeros\"\n",
    "    ),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=(0,1,2)),\n",
    "    SpatialPadd(keys = (\"image\",\"label\"), spatial_size=dims_tuple_image), #ensure all images are (1,256,256,256) if too small\n",
    "    CenterSpatialCropd(keys=(\"image\", \"label\"), roi_size=dims_tuple_image), # ensure all images are (1,256,256,256) if too big\n",
    "    GridPatchd(\n",
    "        keys=(\"image\", \"label\"),\n",
    "        patch_size=(dims_tuple_patches),\n",
    "        offset=(0, 0, 0),\n",
    "        stride=(dims_tuple_patches) # Non-overlapping\n",
    "    ),\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "# view size of image and label for training\n",
    "sample_train = train_transforms(train[0])\n",
    "print(\"Test image shape:\", sample_train[\"image\"].shape)\n",
    "print(\"Test label shape:\", sample_train[\"label\"].shape)\n",
    "\n",
    "# want to validate and test with whole images \n",
    "test_transforms = Compose([\n",
    "    SaveImagePath(keys=[\"image\"]),\n",
    "    LoadImaged(\n",
    "        keys=[\"image\", \"label\"]\n",
    "    ),  # load image\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    ScaleIntensityd(keys=[\"image\"]),\n",
    "    SpatialPadd(keys = (\"image\",\"label\"),spatial_size=dims_tuple_image), # ensure data is the same size\n",
    "    CenterSpatialCropd(keys=(\"image\", \"label\"), roi_size=dims_tuple_image), # ensure all images are (1,256,256,256) if too big\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "sample_test = test_transforms(test[0])\n",
    "print(\"Test image shape:\", sample_test[\"image\"].shape)\n",
    "print(\"Test label shape:\", sample_test[\"label\"].shape)\n",
    "print(\"Image file path:\", sample_test[\"image_filepath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00458dd6",
   "metadata": {},
   "source": [
    "Datsets anf dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade30d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = Dataset(data=train, transform=train_transforms)\n",
    "test_ds = Dataset(data=test, transform=test_transforms)\n",
    "\n",
    "# training, validating, testing of whole data so use a batch size of 1\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0fbae",
   "metadata": {},
   "source": [
    "Define generator unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ad1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels:  [32, 64, 128, 256]\n",
      "strides:  [2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# calculate channels and strides based on given parameters\n",
    "\n",
    "# if depth is 3, and filter is 16, channels = 16, 32, 64\n",
    "filter = 32\n",
    "\n",
    "channels = []\n",
    "for i in range(depth_G):\n",
    "    channels.append(filter)\n",
    "    filter *=2\n",
    "print(\"channels: \", channels)\n",
    "strides = []\n",
    "for i in range(depth_G-1):\n",
    "    strides.append(2)\n",
    "print(\"strides: \", strides)\n",
    "\n",
    "# define model \n",
    "gen_model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=channels,\n",
    "    strides=strides,\n",
    "    num_res_units=2,\n",
    "    dropout=0.2,\n",
    "    norm='BATCH'\n",
    ").apply(monai.networks.normal_init).to(device)\n",
    "\n",
    "trainable_params_gen = sum(p.numel() for p in gen_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ff3ce",
   "metadata": {},
   "source": [
    "Define discriminator model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "187c3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminator(nn.Module):\n",
    "    def __init__(self, ini_filters):\n",
    "        super().__init__()\n",
    "        in_channels=2\n",
    "        kernel_size=3\n",
    "       \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, ini_filters, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(ini_filters, ini_filters*2, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters*2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(ini_filters*2, ini_filters*4, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters*4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_out = nn.Conv3d(ini_filters*4, 1, kernel_size, stride=1, padding=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd6393",
   "metadata": {},
   "source": [
    "GAN loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "499893dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratorLoss(nongad_images,degad_images, fake_preds, device):\n",
    "    \"\"\"\n",
    "    Loss function is the sum of the binary cross entropy between the error of the discriminator output btwn gad and degad (fake prediction) and the root mean square error betwn as nongad and degad images multiplies by scalar weight coefficient\n",
    "    nongad_image= real nongad images from the sample\n",
    "    degad_images= generated nongad images from the generator\n",
    "    fake_preds: output of discriminator when fed fake data\n",
    "    \"\"\"\n",
    "    \n",
    "    coeff = 0.01\n",
    "    \n",
    "    BCE_loss= torch.nn.BCELoss() \n",
    "    real_target = torch.ones((fake_preds.shape[0], fake_preds.shape[1], fake_preds.shape[2], fake_preds.shape[3], fake_preds.shape[4])) #new_full returns a tensor filled with 1 with the same shape as the discrminator prediction \n",
    "    fake_preds = torch.sigmoid(fake_preds) # applying sigmmoid function to output of the discriminator to map probability between 0 and 1\n",
    "    BCE_fake = BCE_loss(fake_preds.to(device), real_target.to(device)) # BCE loss btwn the output of discrim when fed fake data and 1 <- generator wants to minimize this\n",
    "    L1_loss = torch.nn.L1Loss()\n",
    "    loss = L1_loss(degad_images, nongad_images)  # producing RMSE between ground truth nongad and degad\n",
    "    generator_loss = loss*coeff + BCE_fake\n",
    "    return generator_loss\n",
    "\n",
    "def DiscriminatorLoss(real_preds, fake_preds,device):\n",
    "    \"\"\"\n",
    "    Loss function for the discriminator: The discriminator loss is calculated by taking the sum of the L2 error of the discriminator output btwn gad and nongad( real prediction ) and the L2 error of the output btwn gad and degad( fake predition)\n",
    "    \n",
    "    real_preds: output of discriminator when fed real data\n",
    "    fake_preds: output of discriminator when fed fake data\n",
    "    \"\"\"\n",
    "    \n",
    "    real_target = torch.ones((real_preds.shape[0], real_preds.shape[1], real_preds.shape[2],real_preds.shape[3], real_preds.shape[4])) #new_full returns a tensor filled with 1 with the same shape as the discrminator prediction \n",
    "    \n",
    "    fake_target = torch.zeros((fake_preds.shape[0], fake_preds.shape[1], fake_preds.shape[2], fake_preds.shape[3], fake_preds.shape[4])) #new_full returns a tensor filled with 0 w/ the same shape as the generator prediction\n",
    "    BCE_loss =  torch.nn.BCELoss().to(device)  # creates a losss value for each batch, averaging the value across all elements\n",
    "    # Apply sigmoid to discriminator outputs, to fit between 0 and 1\n",
    "    real_preds = torch.sigmoid(real_preds).cuda()\n",
    "    fake_preds = torch.sigmoid(fake_preds).cuda()\n",
    "    \n",
    "    BCE_fake = BCE_loss(fake_preds.cuda(), fake_target.cuda()) # BCE loss btwn the output of discrim when fed fake data and 0 <- generator wants to minimize this\n",
    "    BCE_real = BCE_loss(real_preds.cuda(), real_target.cuda()) # BCE loss btwn the output of discrim when fed real data and 1 <- generator wants to minimize this\n",
    "    \n",
    "    return BCE_real + BCE_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b555f",
   "metadata": {},
   "source": [
    "Define discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3beb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model = GANDiscriminator(filter_num_D).apply(monai.networks.normal_init).to(device)\n",
    "trainable_params_disc = sum(p.numel() for p in disc_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fd6a9",
   "metadata": {},
   "source": [
    "optimizer, steps, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = lr\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen_model.parameters(), lr = learning_rate, betas=betas)\n",
    "disc_opt = torch.optim.Adam(disc_model.parameters(), lr = learning_rate, betas=betas)\n",
    "\n",
    "epoch_loss_values = [float('inf')] # list of generator  loss calculated at the end of each epoch\n",
    "disc_loss_values = [float('inf')] # list of discriminator loss values calculated at end of each epoch\n",
    "disc_train_steps = int(train_steps_d)# number of times to loop thru discriminator for each batch\n",
    "max_epochs = 2\n",
    "\n",
    "loss = torch.nn.L1Loss().to(device)\n",
    "test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9cf2e",
   "metadata": {},
   "source": [
    "Train and validate model loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "1/2 epoch 1, avg gen loss: inf, avg disc loss: inf [===============               ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m disc_real_pred \u001b[38;5;241m=\u001b[39m disc_model(torch\u001b[38;5;241m.\u001b[39mcat([gad_images, nongad_images], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     40\u001b[0m disc_fake_pred \u001b[38;5;241m=\u001b[39m disc_model(torch\u001b[38;5;241m.\u001b[39mcat([gad_images, degad_images], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \n\u001b[0;32m---> 42\u001b[0m disc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mDiscriminatorLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc_real_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdisc_fake_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m disc_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m#initializes back propagation to compute gradient of current tensors \u001b[39;00m\n\u001b[1;32m     44\u001b[0m disc_opt\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# updates parameters to minimize loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mDiscriminatorLoss\u001b[0;34m(real_preds, fake_preds, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m BCE_loss \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# creates a losss value for each batch, averaging the value across all elements\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Apply sigmoid to discriminator outputs, to fit between 0 and 1\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m real_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m fake_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(fake_preds)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     36\u001b[0m BCE_fake \u001b[38;5;241m=\u001b[39m BCE_loss(fake_preds\u001b[38;5;241m.\u001b[39mcuda(), fake_target\u001b[38;5;241m.\u001b[39mcuda()) \u001b[38;5;66;03m# BCE loss btwn the output of discrim when fed fake data and 0 <- generator wants to minimize this\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1648\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1648\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "    gen_model.train()\n",
    "    disc_model.train()\n",
    "    \n",
    "    progress_bar(\n",
    "        index = epoch + 1,\n",
    "        count = max_epochs, \n",
    "        desc = f\"epoch {epoch + 1}, avg gen loss: {epoch_loss_values[-1]:.4f}, avg disc loss: {disc_loss_values[-1]:.4f}\",\n",
    "        newline=True\n",
    "    )\n",
    "\n",
    "    average_train_loss_gen = 0\n",
    "    average_train_loss_disc = 0 \n",
    "    gen_steps = 0\n",
    "    disc_steps = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        gad_images, nongad_images  =batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        \n",
    "        B, P, C, D, H, W = gad_images.shape\n",
    "\n",
    "        for patch_idx in range(P):\n",
    "            patch_gad = gad_images[:, patch_idx]   # [B, C, D, H, W]\n",
    "            patch_nongad = nongad_images[:, patch_idx]\n",
    "        \n",
    "            gen_opt.zero_grad()\n",
    "            # apply generator model on gad images \n",
    "            degad_images = gen_model(gad_images)\n",
    "            # apply discriminator model \n",
    "            disc_fake_pred = disc_model(torch.cat([gad_images, degad_images], dim=1)) # getting disc losses when fed fake images\n",
    "\n",
    "            gen_loss = GeneratorLoss(nongad_images, degad_images, disc_fake_pred,device) # getting generator losses\n",
    "            gen_loss.backward()# computes gradient(derivative) of current tensor, automatically frees part of greaph that creates loss\n",
    "            gen_opt.step() # updates parameters to minimize loss\n",
    "            average_train_loss_gen += gen_loss.item()\n",
    "            gen_steps += 1\n",
    "\n",
    "            for _ in range(disc_train_steps):\n",
    "                gad_images, nongad_images = gad_images.clone().detach(), nongad_images.clone().detach() # need to recall it for each iteration to avoid error message of backpropagation through a graph a second time after gradients have been freed\n",
    "                \n",
    "                degad_images = gen_model(gad_images) # feeding CNN with gad images\n",
    "                \n",
    "                disc_opt.zero_grad() # resetting gradient for discrminator to 0     \n",
    "                disc_real_pred = disc_model(torch.cat([gad_images, nongad_images], dim=1))\n",
    "                disc_fake_pred = disc_model(torch.cat([gad_images, degad_images], dim=1)) \n",
    "                \n",
    "                disc_loss = DiscriminatorLoss(disc_real_pred,disc_fake_pred,device)\n",
    "                disc_loss.backward() #initializes back propagation to compute gradient of current tensors \n",
    "                disc_opt.step() # updates parameters to minimize loss\n",
    "                average_train_loss_disc += disc_loss.item() # taking sum of disc loss for the number of steps for this batch\n",
    "                disc_steps += 1\n",
    "\n",
    "    average_train_loss_gen /= gen_steps\n",
    "    epoch_loss_values.append(average_train_loss_gen)\n",
    "    average_train_loss_disc /= disc_steps\n",
    "    disc_loss_values.append(average_train_loss_disc)\n",
    "    gen_model.eval()\n",
    "\n",
    "\n",
    "torch.save(gen_model.state_dict(), f\"{output_dir}/trained_generator.pt\")\n",
    "torch.save(disc_model.state_dict(), f\"{output_dir}/trained_discriminator.pt\")\n",
    "end = time.time()\n",
    "time = end - start\n",
    "print(\"time for training: \", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c05158",
   "metadata": {},
   "source": [
    "Plot model stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc418d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{output_dir}/model_stats.txt', 'w') as file:  \n",
    "    file.write(f'Training time: {time}\\n') \n",
    "    file.write(f'Number of trainable parameters in generator: {trainable_params_gen}\\n')\n",
    "    file.write(f'Number of trainable parameters in discriminator: {trainable_params_disc}\\n')\n",
    "    file.write(f'generator loss: {epoch_loss_values[-1]} discriminator loss: {disc_loss_values[-1]}')\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(list(range(len(epoch_loss_values))), epoch_loss_values, label=\"Generator Loss\")\n",
    "plt.plot(list(range(len(disc_loss_values))), disc_loss_values , label=\"Discriminator Loss\")\n",
    "plt.grid(True, \"both\", \"both\")\n",
    "plt.title(\"Generator and Discriminator Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_dir}/lossfunction.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458aadc9",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2351682",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.load_state_dict(torch.load(f'{output_dir}/trained_generator.pt', map_location=torch.device('cpu')))\n",
    "gen_model.eval()\n",
    "\n",
    "output_dir_test = Path(output_dir) / \"test\"\n",
    "output_dir_test.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):      \n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        gad_paths = batch[\"image_filepath\"]\n",
    "        degad_images = sliding_window_inference(gad_images, image_size, 1, gen_model)\n",
    "        degad_images = degad_images[:, :, :image_size, :image_size, :image_size]\n",
    "\n",
    "        loss_value = loss(degad_images, nogad_images)\n",
    "\n",
    "        test_loss += loss_value.item()\n",
    "\n",
    "        # to save the output files \n",
    "        # shape[0] gives number of images \n",
    "        for j in range(degad_images.shape[0]):\n",
    "            gad_path = gad_paths[j] # test dictionary image file name\n",
    "            print(gad_path)\n",
    "            gad_nib = nib.load(gad_path)\n",
    "            sub = Path(gad_path).name.split(\"_\")[0]\n",
    "            degad_name = f\"{sub}_acq-degad_T1w.nii.gz\"\n",
    "            \n",
    "            degad_nib = nib.Nifti1Image(\n",
    "                degad_images[j, 0].detach().numpy()*100, \n",
    "                affine=gad_nib.affine,\n",
    "                header=gad_nib.header\n",
    "            )\n",
    "\n",
    "            os.makedirs(f'{output_dir_test}/bids/{sub}/ses-pre/anat', exist_ok=True) # save in bids format\n",
    "            output_path = f'{output_dir_test}/bids/{sub}/ses-pre/anat/{degad_name}'\n",
    "            nib.save(degad_nib, output_path)\n",
    "    \n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
