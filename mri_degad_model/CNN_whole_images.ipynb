{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c244be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: monai in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: nibabel in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (5.3.2)\n",
      "Requirement already satisfied: scikit-learn in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (80.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from monai) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from nibabel) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch monai nibabel scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764c4bc",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cdc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import glob \n",
    "import os\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nibabel.processing import resample_from_to\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Rand3DElasticd,\n",
    "    ScaleIntensityd,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    RandFlipd,\n",
    "    ToTensord,\n",
    "    MapTransform,\n",
    "    Resize\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from monai.utils import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.inferers import sliding_window_inference\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c502ba9",
   "metadata": {},
   "source": [
    "Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5587dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test\n",
    "# patch_size = (16, 32)\n",
    "# batch_size = (32, 64, 128)\n",
    "# lr = (0.0001, 0.001, 0.01)\n",
    "# filter_num = (16, 32, 64)\n",
    "# depth = (3, 4)\n",
    "# num_conv = (2, 3)\n",
    "# loss_func = \"mae\"\n",
    "\n",
    "image_size = 256\n",
    "batch_size = 1\n",
    "lr = 0.0001\n",
    "filter = 64\n",
    "depth = 4\n",
    "loss_func = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b50c4c",
   "metadata": {},
   "source": [
    "create output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "home_dir = Path.home()\n",
    "output_dir = f\"/localscratch/output_whole_images/image-{image_size}_batch-{batch_size}_LR-{lr}_filter-{filter}_depth-{depth}_loss-{loss_func}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f09751",
   "metadata": {},
   "source": [
    "Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46579389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# use gpu if available \n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32da08",
   "metadata": {},
   "source": [
    "Move preprocessed data to one input folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3634f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P004/ses-pre/normalize/sub-P004_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P004/ses-pre/normalize/sub-P004_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P005/ses-pre/normalize/sub-P005_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P005/ses-pre/normalize/sub-P005_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P006/ses-pre/normalize/sub-P006_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P006/ses-pre/normalize/sub-P006_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P007/ses-pre/normalize/sub-P007_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P007/ses-pre/normalize/sub-P007_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P008/ses-pre/normalize/sub-P008_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P008/ses-pre/normalize/sub-P008_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P010/ses-pre/normalize/sub-P010_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P010/ses-pre/normalize/sub-P010_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P011/ses-pre/normalize/sub-P011_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P011/ses-pre/normalize/sub-P011_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P012/ses-pre/normalize/sub-P012_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P012/ses-pre/normalize/sub-P012_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P014/ses-pre/normalize/sub-P014_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P014/ses-pre/normalize/sub-P014_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P015/ses-pre/normalize/sub-P015_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P015/ses-pre/normalize/sub-P015_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P016/ses-pre/normalize/sub-P016_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P016/ses-pre/normalize/sub-P016_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P017/ses-pre/normalize/sub-P017_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P017/ses-pre/normalize/sub-P017_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P018/ses-pre/normalize/sub-P018_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P018/ses-pre/normalize/sub-P018_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P019/ses-pre/normalize/sub-P019_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P019/ses-pre/normalize/sub-P019_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P020/ses-pre/normalize/sub-P020_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P020/ses-pre/normalize/sub-P020_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P021/ses-pre/normalize/sub-P021_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P021/ses-pre/normalize/sub-P021_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P022/ses-pre/normalize/sub-P022_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P022/ses-pre/normalize/sub-P022_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P023/ses-pre/normalize/sub-P023_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P023/ses-pre/normalize/sub-P023_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P024/ses-pre/normalize/sub-P024_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P024/ses-pre/normalize/sub-P024_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P025/ses-pre/normalize/sub-P025_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P025/ses-pre/normalize/sub-P025_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P026/ses-pre/normalize/sub-P026_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P026/ses-pre/normalize/sub-P026_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P028/ses-pre/normalize/sub-P028_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P028/ses-pre/normalize/sub-P028_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P029/ses-pre/normalize/sub-P029_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P029/ses-pre/normalize/sub-P029_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P030/ses-pre/normalize/sub-P030_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P030/ses-pre/normalize/sub-P030_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P031/ses-pre/normalize/sub-P031_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P031/ses-pre/normalize/sub-P031_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P032/ses-pre/normalize/sub-P032_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P032/ses-pre/normalize/sub-P032_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P033/ses-pre/normalize/sub-P033_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P033/ses-pre/normalize/sub-P033_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P034/ses-pre/normalize/sub-P034_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P034/ses-pre/normalize/sub-P034_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P035/ses-pre/normalize/sub-P035_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P035/ses-pre/normalize/sub-P035_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P036/ses-pre/normalize/sub-P036_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P036/ses-pre/normalize/sub-P036_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P037/ses-pre/normalize/sub-P037_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P037/ses-pre/normalize/sub-P037_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P038/ses-pre/normalize/sub-P038_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P038/ses-pre/normalize/sub-P038_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P039/ses-pre/normalize/sub-P039_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P039/ses-pre/normalize/sub-P039_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P041/ses-pre/normalize/sub-P041_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P041/ses-pre/normalize/sub-P041_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P042/ses-pre/normalize/sub-P042_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P042/ses-pre/normalize/sub-P042_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P043/ses-pre/normalize/sub-P043_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P043/ses-pre/normalize/sub-P043_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P044/ses-pre/normalize/sub-P044_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P044/ses-pre/normalize/sub-P044_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P045/ses-pre/normalize/sub-P045_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P045/ses-pre/normalize/sub-P045_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P046/ses-pre/normalize/sub-P046_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P046/ses-pre/normalize/sub-P046_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P047/ses-pre/normalize/sub-P047_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P047/ses-pre/normalize/sub-P047_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P048/ses-pre/normalize/sub-P048_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P048/ses-pre/normalize/sub-P048_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P051/ses-pre/normalize/sub-P051_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P051/ses-pre/normalize/sub-P051_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P052/ses-pre/normalize/sub-P052_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P052/ses-pre/normalize/sub-P052_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P053/ses-pre/normalize/sub-P053_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P053/ses-pre/normalize/sub-P053_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P054/ses-pre/normalize/sub-P054_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P054/ses-pre/normalize/sub-P054_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P055/ses-pre/normalize/sub-P055_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P055/ses-pre/normalize/sub-P055_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P056/ses-pre/normalize/sub-P056_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P056/ses-pre/normalize/sub-P056_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P057/ses-pre/normalize/sub-P057_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P057/ses-pre/normalize/sub-P057_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P058/ses-pre/normalize/sub-P058_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P058/ses-pre/normalize/sub-P058_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P059/ses-pre/normalize/sub-P059_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P059/ses-pre/normalize/sub-P059_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P060/ses-pre/normalize/sub-P060_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P060/ses-pre/normalize/sub-P060_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P061/ses-pre/normalize/sub-P061_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P061/ses-pre/normalize/sub-P061_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P062/ses-pre/normalize/sub-P062_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P062/ses-pre/normalize/sub-P062_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P063/ses-pre/normalize/sub-P063_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P063/ses-pre/normalize/sub-P063_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P064/ses-pre/normalize/sub-P064_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P064/ses-pre/normalize/sub-P064_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P065/ses-pre/normalize/sub-P065_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P065/ses-pre/normalize/sub-P065_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P066/ses-pre/normalize/sub-P066_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P066/ses-pre/normalize/sub-P066_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P068/ses-pre/normalize/sub-P068_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P068/ses-pre/normalize/sub-P068_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Found gad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P069/ses-pre/normalize/sub-P069_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz']\n",
      "Found nogad images: ['/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P069/ses-pre/normalize/sub-P069_ses-pre_acq-nongad_run-01_desc-normalized_zscore_T1w.nii.gz']\n",
      "Loaded 60 paired samples.\n"
     ]
    }
   ],
   "source": [
    "# creates a dictionary of pairs of image file paths (str)\n",
    "\n",
    "input_dir = os.path.expanduser(\"~/graham/scratch/degad_preprocessed_data\")\n",
    "\n",
    "work_dir = os.path.join(input_dir, \"work\")\n",
    "subject_dirs = glob.glob(os.path.join(work_dir, \"sub-*\"))\n",
    "\n",
    "subjects = []\n",
    "for directory in subject_dirs:\n",
    "    if os.path.isdir(directory): \n",
    "        subjects.append(directory)\n",
    "\n",
    "data_dicts = []\n",
    "for sub in subjects:   \n",
    "    gad_images = glob.glob(os.path.join(sub, \"ses-pre\", \"normalize\", \"*acq-gad*_T1w.nii.gz\"))\n",
    "    print(\"Found gad images:\", gad_images)\n",
    "    \n",
    "    nogad_images = glob.glob(os.path.join(sub, \"ses-pre\", \"normalize\", \"*acq-nongad*_T1w.nii.gz\"))\n",
    "    print(\"Found nogad images:\", nogad_images)\n",
    "    \n",
    "    if gad_images and nogad_images:\n",
    "        data_dicts.append({\"image\": gad_images[0], \"label\": nogad_images[0], \"image_filepath\": gad_images[0]})\n",
    "\n",
    "print(\"Loaded\", len(data_dicts), \"paired samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699898c6",
   "metadata": {},
   "source": [
    "Split into train, test, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da63673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 42, Val: 9, Test: 9\n"
     ]
    }
   ],
   "source": [
    "# create split of the image path strings\n",
    "\n",
    "# 70% train, 15% val, 15% test \n",
    "\n",
    "train_val, test = train_test_split(data_dicts, test_size=0.15, random_state=42)\n",
    "\n",
    "# 0.176 ≈ 15% of the full data\n",
    "train, val = train_test_split(train_val, test_size=0.176, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7888b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveImagePath(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # Storing the file path separately in the 'image_filepath' key\n",
    "        data['image_filepath'] = data['image']\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4da5e",
   "metadata": {},
   "source": [
    "Define transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bab80e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims_tuple:  (256, 256, 256)\n",
      "Test image shape: torch.Size([1, 256, 256, 256])\n",
      "Test label shape: torch.Size([1, 256, 256, 256])\n",
      "Val image shape: torch.Size([1, 256, 276, 256])\n",
      "Val label shape: torch.Size([1, 256, 276, 256])\n",
      "Test image shape: torch.Size([1, 261, 263, 256])\n",
      "Test label shape: torch.Size([1, 261, 263, 256])\n",
      "Image file path: /home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# using transformations from original code \n",
    "\n",
    "# set size of image to patch size (patch_size, patch_size, patch_size)\n",
    "dims_tuple = (image_size,)*3\n",
    "print(\"dims_tuple: \", dims_tuple)\n",
    "\n",
    "# train tranforms \n",
    "train_transforms = Compose([\n",
    "    LoadImaged(\n",
    "        keys=[\"image\", \"label\"], \n",
    "    ),  # load image from the file path \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]), # ensure this is [C, H, W, (D)]\n",
    "    ScaleIntensityd(keys=[\"image\"]), # scales the intensity from 0-1\n",
    "    Rand3DElasticd(\n",
    "        keys = (\"image\",\"label\"), \n",
    "        sigma_range = (0.5,1), \n",
    "        magnitude_range = (0.1, 0.4), \n",
    "        prob=0.4, \n",
    "        shear_range=(0.1, -0.05, 0.0, 0.0, 0.0, 0.0),\n",
    "        scale_range=0.5, padding_mode= \"zeros\"\n",
    "    ),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=(0,1,2)),\n",
    "    SpatialPadd(keys = (\"image\",\"label\"), spatial_size=dims_tuple), #ensure all images are (1,256,256,256) if too small\n",
    "    CenterSpatialCropd(keys=(\"image\", \"label\"), roi_size=dims_tuple), # ensure all images are (1,256,256,256) if too big\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "# view size of image and label for training\n",
    "sample_train = train_transforms(train[0])\n",
    "print(\"Test image shape:\", sample_train[\"image\"].shape)\n",
    "print(\"Test label shape:\", sample_train[\"label\"].shape)\n",
    "\n",
    "# want to validate and test with whole images \n",
    "val_transforms = Compose([\n",
    "    SaveImagePath(keys=[\"image\"]),\n",
    "    LoadImaged(\n",
    "        keys=[\"image\", \"label\"]\n",
    "    ),  # load image\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    ScaleIntensityd(keys=[\"image\"]),\n",
    "    SpatialPadd(keys = (\"image\",\"label\"),spatial_size=dims_tuple), # pad data to be 256\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "sample_val = val_transforms(val[0])\n",
    "print(\"Val image shape:\", sample_val[\"image\"].shape)\n",
    "print(\"Val label shape:\", sample_val[\"label\"].shape)\n",
    "\n",
    "sample_test = val_transforms(test[0])\n",
    "print(\"Test image shape:\", sample_test[\"image\"].shape)\n",
    "print(\"Test label shape:\", sample_test[\"label\"].shape)\n",
    "print(\"Image file path:\", sample_test[\"image_filepath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a34313",
   "metadata": {},
   "source": [
    "Set up datasets and data loader with monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef550ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = Dataset(data=train, transform=train_transforms)\n",
    "val_ds = Dataset(data=val, transform=val_transforms)\n",
    "test_ds = Dataset(data=test, transform=val_transforms)\n",
    "\n",
    "# training, validating, testing of whole data so use a batch size of 1\n",
    "train_loader = DataLoader(train_ds, batch_size=len(train), shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31a7adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage shape:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLabel shape:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:1135\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1132\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(\"Image shape:\", batch[\"image\"].shape)\n",
    "    print(\"Label shape:\", batch[\"label\"].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a3e4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_batch(loader, num_samples=6, title=\"Batch Samples\"):\n",
    "\n",
    "#     plt.figure(figsize=(12, 2 * num_samples))\n",
    "    \n",
    "#     for i, batch in enumerate(loader):\n",
    "#         images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "#         # convert to numpy arrays (remove channel dimension)\n",
    "#         images = images.numpy()\n",
    "#         labels = labels.numpy()\n",
    "\n",
    "#         batch_size = images.shape[0]\n",
    "#         max_to_show = min(num_samples, batch_size)\n",
    "\n",
    "#         for idx in range(max_to_show):\n",
    "#             img = np.squeeze(images[idx])  # shape: (H, W, D) or (D, H, W)\n",
    "#             lbl = np.squeeze(labels[idx])\n",
    "\n",
    "#             # pick a slice along the last dimension\n",
    "#             slice_index = img.shape[-1] // 2\n",
    "\n",
    "#             # Show image\n",
    "#             plt.subplot(max_to_show, 2, idx * 2 + 1)\n",
    "#             plt.imshow(img[..., slice_index], cmap=\"gray\")\n",
    "#             plt.title(f\"Image {idx+1}\")\n",
    "#             plt.axis(\"off\")\n",
    "\n",
    "#             # Show label\n",
    "#             plt.subplot(max_to_show, 2, idx * 2 + 2)\n",
    "#             plt.imshow(lbl[..., slice_index], cmap=\"gray\")\n",
    "#             plt.title(f\"Label {idx+1}\")\n",
    "#             plt.axis(\"off\")\n",
    "\n",
    "#     plt.suptitle(title)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# visualize_batch(train_loader, num_samples=6, title=\"Train Batch Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e81649",
   "metadata": {},
   "source": [
    "Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81bee68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels:  [64, 128, 256, 512]\n",
      "strides:  [2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# calculate channels and strides based on given parameters\n",
    "channels = []\n",
    "for i in range(depth):\n",
    "    channels.append(filter)\n",
    "    filter *=2\n",
    "print(\"channels: \", channels)\n",
    "strides = []\n",
    "for i in range(depth-1):\n",
    "    strides.append(2)\n",
    "print(\"strides: \", strides)\n",
    "\n",
    "# define model \n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=channels,\n",
    "    strides=strides,\n",
    "    num_res_units=2,\n",
    "    dropout=0.2,\n",
    "    norm='BATCH'\n",
    ").apply(monai.networks.normal_init).to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea381b",
   "metadata": {},
   "source": [
    "Define lr, optimization, epochs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e835a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = float(lr)\n",
    "\n",
    "# common defaults\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas=betas)\n",
    "\n",
    "best_model_path = f\"{output_dir}/best_model.pt\"\n",
    "max_epochs = 1\n",
    "patience = 22\n",
    "\n",
    "loss = torch.nn.L1Loss().to(device)\n",
    "\n",
    "train_losses = [float('inf')]\n",
    "val_losses = [float('inf')]\n",
    "best_val_loss = float('inf')\n",
    "test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a74d64",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b7898ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "1/1 epoch 1, training mae loss: inf, validation mae metric: inf [==============================]\n",
      "\n",
      "--------Training--------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "\n",
    "    train_loss_display = f\"{train_losses[-1]:.4f}\" if train_losses else \"N/A\"\n",
    "    val_loss_display = f\"{val_losses[-1]:.4f}\" if val_losses else \"N/A\"\n",
    "\n",
    "    progress_bar(\n",
    "        index=epoch + 1,\n",
    "        count=max_epochs,\n",
    "        desc=f\"epoch {epoch + 1}, training mae loss: {train_loss_display}, validation mae metric: {val_loss_display}\",\n",
    "        newline=True\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    avg_train_loss = 0\n",
    "\n",
    "    print(\"--------Training--------\")\n",
    "    for batch in train_loader:\n",
    "        # image is gad image, label is nogad image\n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        optimizer.zero_grad() #resets optimizer to 0\n",
    "        degad_images = model(gad_images)\n",
    "        degad_images = degad_images[:, :, :image_size, :image_size, :image_size]\n",
    "        \n",
    "        train_loss = loss(degad_images, nogad_images)\n",
    "        train_loss.backward() # computes gradients for each parameter based on loss\n",
    "        optimizer.step() # updates the model weights using the gradient\n",
    "        avg_train_loss += train_loss.item() \n",
    "   \n",
    "    avg_train_loss /= len(train_loader) # average loss per current epoch \n",
    "    train_losses.append(avg_train_loss) # append total epoch loss divided by the number of training steps in epoch to loss list\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"--------Validation--------\")\n",
    "    # validation \n",
    "    with torch.no_grad(): #we do not update weights/biases in validation training, only used to assess current state of model\n",
    "        avg_val_loss = 0 # will hold sum of all validation losses in epoch and then average\n",
    "        for batch in val_loader: # iterating through dataloader\n",
    "            gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            degad_images = model(gad_images)\n",
    "            degad_images = degad_images[:, :, :image_size, :image_size, :image_size]\n",
    "\n",
    "            val_loss = loss(degad_images, nogad_images)\n",
    "            avg_val_loss += val_loss.item() \n",
    "        \n",
    "        avg_val_loss /= len(val_loader)  # Average validation loss for the epoch\n",
    "        val_losses.append(avg_val_loss) \n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model.\")\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement in validation loss. Best remains {best_val_loss:.4f}.\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "        \n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(\"time for training and validation: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e97b9b",
   "metadata": {},
   "source": [
    "Plot training metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from original code \n",
    "\n",
    "with open (f'{output_dir}/model_stats.txt', 'w') as file:  \n",
    "    file.write(f'Training time: {total_time:.2f} seconds\\n') \n",
    "    file.write(f'Number of trainable parameters: {trainable_params}\\n')\n",
    "\n",
    "    if len(train_losses) > patience:\n",
    "        file.write(f'Training loss (epoch {-patience}): {train_losses[-patience]:.4f}\\n')\n",
    "    else:\n",
    "        file.write(f'Training loss (last epoch): {train_losses[-1]:.4f}\\n')\n",
    "\n",
    "    file.write(f'Validation loss (best): {best_val_loss:.4f}\\n')\n",
    "    \n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/lossfunction.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0c76a",
   "metadata": {},
   "source": [
    "Test model with sliding window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/UWO/msnyde26/graham/scratch/degad_preprocessed_data/work/sub-P003/ses-pre/normalize/sub-P003_ses-pre_acq-gad_run-01_desc-normalize_minmax_T1w.nii.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[31mTypeError\u001b[39m: float() argument must be a string or a real number, not 'Nifti1Image'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m data_np = degad_images[j, \u001b[32m0\u001b[39m].detach().cpu().numpy()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Create prediction Nifti in transformed space\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m pred_nib = \u001b[43mnib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNifti1Image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgad_nib\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# temporary identity affine\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Resample prediction to match original GAD image (both shape & affine)\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# resampled_nib = resample_from_to(pred_nib, gad_nib)\u001b[39;00m\n\u001b[32m     39\u001b[39m os.makedirs(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/bids/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/ses-pre/anat\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# save in bids format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/nibabel/nifti1.py:2006\u001b[39m, in \u001b[36mNifti1Pair.__init__\u001b[39m\u001b[34m(self, dataobj, affine, header, extra, file_map, dtype)\u001b[39m\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_obj_dtype(dataobj) \u001b[38;5;129;01min\u001b[39;00m danger_dts:\n\u001b[32m   1996\u001b[39m     alert_future_error(\n\u001b[32m   1997\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mImage data has type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataobj.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, which may cause \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1998\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mincompatibilities with other tools.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2004\u001b[39m         error_class=\u001b[38;5;167;01mValueError\u001b[39;00m,\n\u001b[32m   2005\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2006\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[38;5;66;03m# Force set of s/q form when header is None unless affine is also None\u001b[39;00m\n\u001b[32m   2008\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m affine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/nibabel/analyze.py:912\u001b[39m, in \u001b[36mAnalyzeImage.__init__\u001b[39m\u001b[34m(self, dataobj, affine, header, extra, file_map, dtype)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataobj, affine, header=\u001b[38;5;28;01mNone\u001b[39;00m, extra=\u001b[38;5;28;01mNone\u001b[39;00m, file_map=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m     \u001b[38;5;66;03m# Reset consumable values\u001b[39;00m\n\u001b[32m    914\u001b[39m     \u001b[38;5;28mself\u001b[39m._header.set_data_offset(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/MRI_Degad_Model/venv/lib/python3.12/site-packages/nibabel/spatialimages.py:513\u001b[39m, in \u001b[36mSpatialImage.__init__\u001b[39m\u001b[34m(self, dataobj, affine, header, extra, file_map)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(dataobj, header=header, extra=extra, file_map=file_map)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m affine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    508\u001b[39m     \u001b[38;5;66;03m# Check that affine is array-like 4,4.  Maybe this is too strict at\u001b[39;00m\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# this abstract level, but so far I think all image formats we know\u001b[39;00m\n\u001b[32m    510\u001b[39m     \u001b[38;5;66;03m# do need 4,4.\u001b[39;00m\n\u001b[32m    511\u001b[39m     \u001b[38;5;66;03m# Copy affine to isolate from environment.  Specify float type to\u001b[39;00m\n\u001b[32m    512\u001b[39m     \u001b[38;5;66;03m# avoid surprising integer rounding when setting values into affine\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     affine = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m affine.shape == (\u001b[32m4\u001b[39m, \u001b[32m4\u001b[39m):\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mAffine should be shape 4,4\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "test_loss = 0 \n",
    "\n",
    "model.load_state_dict(torch.load(f'{output_dir}/best_model.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "output_dir_test = Path(output_dir) / \"test\"\n",
    "output_dir_test.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):      \n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        gad_paths = batch[\"image_filepath\"]\n",
    "        degad_images = sliding_window_inference(gad_images, image_size, 1, model)\n",
    "        degad_images = degad_images[:, :, :image_size, :image_size, :image_size]\n",
    "\n",
    "        loss_value = loss(degad_images, nogad_images)\n",
    "\n",
    "        test_loss += loss_value.item()\n",
    "\n",
    "        # to save the output files \n",
    "        # shape[0] gives number of images \n",
    "        for j in range(0,gad_images.shape[0]):\n",
    "            gad_path = gad_paths[j] # test dictionary image file name\n",
    "            print(gad_path)\n",
    "            gad_nib = nib.load(gad_path)\n",
    "            sub = Path(gad_path).name.split(\"_\")[0]\n",
    "            degad_name = f\"{sub}_acq-degad_T1w.nii.gz\"\n",
    "            \n",
    "            # Convert predicted output to NumPy\n",
    "            data_np = degad_images[j, 0].detach().cpu().numpy()\n",
    "\n",
    "            # Create prediction Nifti in transformed space\n",
    "            pred_nib = nib.Nifti1Image(data_np, affine=np.eye(4))  # temporary identity affine\n",
    "\n",
    "            # Resample prediction to match original GAD image (both shape & affine)\n",
    "            resampled_nib = resample_from_to(pred_nib, gad_nib)\n",
    "\n",
    "\n",
    "            os.makedirs(f'{output_dir_test}/bids/{sub}/ses-pre/anat', exist_ok=True) # save in bids format\n",
    "            output_path = f'{output_dir_test}/bids/{sub}/ses-pre/anat/{degad_name}'\n",
    "            nib.save(resampled_nib, output_path)\n",
    "    \n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
