{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch monai nibabel scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19feac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Rand3DElasticd,\n",
    "    SpatialPadd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd,\n",
    "    ToTensord\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "import torch.nn as nn\n",
    "import time \n",
    "\n",
    "from monai.utils import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.inferers import sliding_window_inference\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f18c9",
   "metadata": {},
   "source": [
    "changable model params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 32\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "filter_num_G = 32\n",
    "filter_num_D = 32\n",
    "depth_G = 3\n",
    "train_steps_d = 3\n",
    "loss_func = \"mae\"\n",
    "\n",
    "# patch_size = (16, 32)\n",
    "# batch_size = (32, 64, 128)\n",
    "# lr = (0.0001, 0.001, 0.01)\n",
    "# filter_num_G = (16, 32, 64)\n",
    "# filter_num_D = (16, 32, 64)\n",
    "# depth_G = (3, 4)\n",
    "# train_steps_d = (3,4)\n",
    "# loss_func = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224584be",
   "metadata": {},
   "source": [
    "define model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e00d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"output/patch-{patch_size}_batch-{batch_size}_LR-{lr}_filter_G-{filter_num_G}_filter_D-{filter_num_D}_depth_G-{depth_G}_train_steps_d_{train_steps_d}_loss_func_{loss_func}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c07a6",
   "metadata": {},
   "source": [
    "define available resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available \n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08334c",
   "metadata": {},
   "source": [
    "organize data dict for model inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1746432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bids input folder to subject folders from preprocessing output \n",
    "bids_root = \"user_input\"\n",
    "\n",
    "# subject folders \n",
    "subjects = sorted(glob.glob(os.path.join(bids_root, \"sub-*\")))\n",
    "\n",
    "data_dicts = []\n",
    "\n",
    "# create a dictonary of matching gad and nogad files\n",
    "for sub in subjects:\n",
    "    gad_images = glob.glob(os.path.join(sub, \"work\", \"ses-pre\", \"anat\", \"*_gad*_T1w.nii.gz\"))\n",
    "    nogad_images = glob.glob(os.path.join(sub, \"work\", \"ses-pre\", \"anat\", \"*nongad*_T1w.nii.gz\"))\n",
    "    if gad_images and nogad_images:\n",
    "        data_dicts.append({\"image\": gad_images[0], \"label\": nogad_images[0]})\n",
    "\n",
    "print(\"Loaded\", len(data_dicts), \"paired samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4f263",
   "metadata": {},
   "source": [
    "split into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cce638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_dicts, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceae697",
   "metadata": {},
   "source": [
    "Data transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transformations from original code \n",
    "\n",
    "# set size of image to patch size (patch_size, patch_size, patch_size)\n",
    "dims_tuple = (patch_size,)*3\n",
    "\n",
    "# want to train with patches\n",
    "train_transforms = Compose([\n",
    "    SpatialPadd(keys = (\"image\",\"label\"), spatial_size = dims_tuple), #ensures all data is around the same size\n",
    "    Rand3DElasticd(keys = (\"image\",\"label\"), sigma_range = (0.5,1), magnitude_range = (0.1, 0.4), prob=0.4, shear_range=(0.1, -0.05, 0.0, 0.0, 0.0, 0.0), scale_range=0.5, padding_mode= \"zeros\"),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=1),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=0),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=2),\n",
    "    RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=patch_size, random_center=True, random_size=False),\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "# want to validate and test with whole images \n",
    "test_transforms = Compose([\n",
    "    SpatialPadd(keys = (\"image\",\"label\"),spatial_size = dims_tuple),\n",
    "    ToTensord(keys=[\"image\", \"label\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00458dd6",
   "metadata": {},
   "source": [
    "Datsets anf dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade30d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(data=train, transform=train_transforms)\n",
    "test_ds = Dataset(data=test, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=pin_memory)\n",
    "\n",
    "# test on whole brain data \n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0fbae",
   "metadata": {},
   "source": [
    "Define generator unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate channels and strides based on given parameters\n",
    "\n",
    "# if depth is 3, and filter is 16, channels = 16, 32, 64\n",
    "channels = []\n",
    "for i in range(depth_G):\n",
    "    channels.append(filter_num_G)\n",
    "    filter_num_G *=2\n",
    "print(\"channels: \", channels)\n",
    "\n",
    "# if depth is 3, strides = 2, 2, 1 \n",
    "strides = []\n",
    "for i in range(depth_G - 1):\n",
    "    strides.append(2)\n",
    "strides.append(1)\n",
    "print(\"strides: \", strides)\n",
    "\n",
    "# define model \n",
    "gen_model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=channels,\n",
    "    strides=strides,\n",
    "    num_res_units=2,\n",
    "    dropout=0.2,\n",
    "    norm='BATCH'\n",
    ").apply(monai.networks.normal_init).to(device)\n",
    "\n",
    "trainable_params_gen = sum(p.numel() for p in gen_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ff3ce",
   "metadata": {},
   "source": [
    "Define discriminator model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminator(nn.Module):\n",
    "    def __init__(self, ini_filters):\n",
    "        super().__init__()\n",
    "        in_channels=2\n",
    "        kernel_size=3\n",
    "       \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, ini_filters, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(ini_filters, ini_filters*2, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters*2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(ini_filters*2, ini_filters*4, kernel_size, stride=2, padding=1),\n",
    "            nn.InstanceNorm3d(ini_filters*4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_out = nn.Conv3d(ini_filters*4, 1, kernel_size, stride=1, padding=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd6393",
   "metadata": {},
   "source": [
    "GAN loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499893dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratorLoss(nongad_images,degad_images, fake_preds, device):\n",
    "    \"\"\"\n",
    "    Loss function is the sum of the binary cross entropy between the error of the discriminator output btwn gad and degad (fake prediction) and the root mean square error betwn as nongad and degad images multiplies by scalar weight coefficient\n",
    "    nongad_image= real nongad images from the sample\n",
    "    degad_images= generated nongad images from the generator\n",
    "    fake_preds: output of discriminator when fed fake data\n",
    "    \"\"\"\n",
    "    \n",
    "    coeff = 0.01\n",
    "    \n",
    "    BCE_loss= torch.nn.BCELoss() \n",
    "    real_target = torch.ones((fake_preds.shape[0], fake_preds.shape[1], fake_preds.shape[2], fake_preds.shape[3], fake_preds.shape[4])) #new_full returns a tensor filled with 1 with the same shape as the discrminator prediction \n",
    "    fake_preds = torch.sigmoid(fake_preds) # applying sigmmoid function to output of the discriminator to map probability between 0 and 1\n",
    "    BCE_fake = BCE_loss(fake_preds.to(device), real_target.to(device)) # BCE loss btwn the output of discrim when fed fake data and 1 <- generator wants to minimize this\n",
    "    L1_loss = torch.nn.L1Loss()\n",
    "    loss = L1_loss(degad_images, nongad_images)  # producing RMSE between ground truth nongad and degad\n",
    "    generator_loss = loss*coeff + BCE_fake\n",
    "    return generator_loss\n",
    "\n",
    "def DiscriminatorLoss(real_preds, fake_preds,device):\n",
    "    \"\"\"\n",
    "    Loss function for the discriminator: The discriminator loss is calculated by taking the sum of the L2 error of the discriminator output btwn gad and nongad( real prediction ) and the L2 error of the output btwn gad and degad( fake predition)\n",
    "    \n",
    "    real_preds: output of discriminator when fed real data\n",
    "    fake_preds: output of discriminator when fed fake data\n",
    "    \"\"\"\n",
    "    \n",
    "    real_target = torch.ones((real_preds.shape[0], real_preds.shape[1], real_preds.shape[2],real_preds.shape[3], real_preds.shape[4])) #new_full returns a tensor filled with 1 with the same shape as the discrminator prediction \n",
    "    \n",
    "    fake_target = torch.zeros((fake_preds.shape[0], fake_preds.shape[1], fake_preds.shape[2], fake_preds.shape[3], fake_preds.shape[4])) #new_full returns a tensor filled with 0 w/ the same shape as the generator prediction\n",
    "    BCE_loss =  torch.nn.BCELoss().to(device)  # creates a losss value for each batch, averaging the value across all elements\n",
    "    # Apply sigmoid to discriminator outputs, to fit between 0 and 1\n",
    "    real_preds = torch.sigmoid(real_preds).cuda()\n",
    "    fake_preds = torch.sigmoid(fake_preds).cuda()\n",
    "    \n",
    "    BCE_fake = BCE_loss(fake_preds.cuda(), fake_target.cuda()) # BCE loss btwn the output of discrim when fed fake data and 0 <- generator wants to minimize this\n",
    "    BCE_real = BCE_loss(real_preds.cuda(), real_target.cuda()) # BCE loss btwn the output of discrim when fed real data and 1 <- generator wants to minimize this\n",
    "    \n",
    "    return BCE_real + BCE_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b555f",
   "metadata": {},
   "source": [
    "Define discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3beb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model = GANDiscriminator(filter_num_D).apply(monai.networks.normal_init).to(device)\n",
    "trainable_params_disc = sum(p.numel() for p in disc_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fd6a9",
   "metadata": {},
   "source": [
    "optimizer, steps, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = lr\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen_model.parameters(), lr = learning_rate, betas=betas)\n",
    "disc_opt = torch.optim.Adam(disc_model.parameters(), lr = learning_rate, betas=betas)\n",
    "\n",
    "epoch_loss_values = [float('inf')] # list of generator  loss calculated at the end of each epoch\n",
    "disc_loss_values = [float('inf')] # list of discriminator loss values calculated at end of each epoch\n",
    "disc_train_steps = int(train_steps_d)# number of times to loop thru discriminator for each batch\n",
    "\n",
    "gen_training_steps = int(train_loader / batch_size) # batch_size is a tunable param\n",
    "disc_training_steps = disc_train_steps * gen_training_steps # number of validation steps per epoch\n",
    "\n",
    "max_epochs = 250\n",
    "\n",
    "loss = torch.nn.L1Loss().to(device)\n",
    "test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9cf2e",
   "metadata": {},
   "source": [
    "Train and validate model loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "    gen_model.train()\n",
    "    disc_model.train()\n",
    "    \n",
    "    progress_bar(\n",
    "        index = epoch + 1,\n",
    "        count = max_epochs, \n",
    "        desc = f\"epoch {epoch + 1}, avg gen loss: {epoch_loss_values[-1]:.4f}, avg disc loss: {disc_loss_values[-1]:.4f}\",\n",
    "        newline=True\n",
    "    )\n",
    "\n",
    "    average_train_loss_gen = 0\n",
    "    average_train_loss_disc = 0 \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        gad_images, nongad_images  =batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        # apply generator model on gad images \n",
    "        degad_images = gen_model(gad_images)\n",
    "\n",
    "        # apply discriminator model \n",
    "        disc_fake_pred = disc_model(torch.cat([gad_images, degad_images], dim=1)) # getting disc losses when fed fake images\n",
    "\n",
    "        gen_loss = GeneratorLoss(nongad_images, degad_images, disc_fake_pred,device) # getting generator losses\n",
    "        gen_loss.backward()# computes gradient(derivative) of current tensor, automatically frees part of greaph that creates loss\n",
    "        gen_opt.step() # updates parameters to minimize loss\n",
    "        average_train_loss_gen += gen_loss.item()\n",
    "\n",
    "        for _ in range(disc_train_steps):\n",
    "            gad_images, nongad_images = gad_images.clone().detach(), nongad_images.clone().detach() # need to recall it for each iteration to avoid error message of backpropagation through a graph a second time after gradients have been freed\n",
    "            \n",
    "            degad_images = gen_model(gad_images) # feeding CNN with gad images\n",
    "            \n",
    "            disc_opt.zero_grad() # resetting gradient for discrminator to 0\n",
    "            \n",
    "            disc_real_pred = disc_model(torch.cat([gad_images, nongad_images], dim=1))\n",
    "            disc_fake_pred = disc_model(torch.cat([gad_images, degad_images], dim=1)) \n",
    "            \n",
    "            disc_loss = DiscriminatorLoss(disc_real_pred,disc_fake_pred,device)\n",
    "            disc_loss.backward() #initializes back propagation to compute gradient of current tensors \n",
    "            disc_opt.step() # updates parameters to minimize loss\n",
    "            average_train_loss_disc += disc_loss.item() # taking sum of disc loss for the number of steps for this batch\n",
    "\n",
    "    average_train_loss_gen /= gen_training_steps # epoch loss is the total loss by the end of that epoch divided by the number of steps\n",
    "    epoch_loss_values.append(average_train_loss_gen) #updates the loss value for that epoch\n",
    "    average_train_loss_disc /= disc_training_steps# average disc epoch loss is the total loss divided by the number of discriminator steps\n",
    "    disc_loss_values.append(average_train_loss_disc) # av\n",
    "    gen_model.eval()\n",
    "\n",
    "torch.save(gen_model.state_dict(), f\"{output_dir}/trained_generator.pt\")\n",
    "torch.save(disc_model.state_dict(), f\"{output_dir}/trained_discriminator.pt\")\n",
    "end = time.time()\n",
    "time = end - start\n",
    "print(\"time for training: \", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c05158",
   "metadata": {},
   "source": [
    "Plot model stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc418d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{output_dir}/model_stats.txt', 'w') as file:  \n",
    "    file.write(f'Training time: {time}\\n') \n",
    "    file.write(f'Number of trainable parameters in generator: {trainable_params_gen}\\n')\n",
    "    file.write(f'Number of trainable parameters in discriminator: {trainable_params_disc}\\n')\n",
    "    file.write(f'generator loss: {epoch_loss_values[-1]} discriminator loss: {disc_loss_values[-1]}')\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(list(range(len(epoch_loss_values))), epoch_loss_values, label=\"Generator Loss\")\n",
    "plt.plot(list(range(len(disc_loss_values))), disc_loss_values , label=\"Discriminator Loss\")\n",
    "plt.grid(True, \"both\", \"both\")\n",
    "plt.title(\"Generator and Discriminator Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_dir}/lossfunction.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458aadc9",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2351682",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.load_state_dict(torch.load(f'{output_dir}/trained_generator.pt'))\n",
    "gen_model.eval()\n",
    "\n",
    "output_dir_test = Path(output_dir) / \"test\"\n",
    "output_dir_test.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "\n",
    "        # Sliding window inference for large 3D volumes\n",
    "        degad_images = sliding_window_inference(gad_images, patch_size, 1, gen_model)\n",
    "\n",
    "        loss_value = loss(degad_images, nogad_images)\n",
    "        test_loss += loss_value.item()\n",
    "\n",
    "        # Save degad images as NIfTI\n",
    "        for i in range(degad_images.shape[0]):\n",
    "            gad_path = batch[\"image_meta_dict\"][\"filename_or_obj\"][i]\n",
    "            gad_nib = nib.load(gad_path)\n",
    "            sub = Path(gad_path).name.split(\"_\")[0]\n",
    "            degad_name = f\"{sub}_acq-degad_T1w.nii.gz\"\n",
    "\n",
    "            degad_nib = nib.Nifti1Image(\n",
    "                degad_images[i, 0].detach().numpy()*100,\n",
    "                affine=gad_nib.affine,\n",
    "                header=gad_nib.header,\n",
    "            )\n",
    "\n",
    "            os.makedirs(f'{output_dir_test}/bids/{sub}/ses-pre/anat', exist_ok=True) # save in bids format\n",
    "            output_path = f'{output_dir_test}/bids/{sub}/ses-pre/anat/{degad_name}'\n",
    "            nib.save(degad_nib, output_path)\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
