{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c244be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting monai\n",
      "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nibabel in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (5.3.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-macosx_10_12_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from monai) (1.26.4)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from nibabel) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from nibabel) (24.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-macosx_14_0_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mackenziesnyder/Desktop/MRI_Degad_Model/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp310-cp310-macosx_10_12_x86_64.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl (268 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp310-cp310-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-macosx_10_9_x86_64.whl (66 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading scipy-1.15.2-cp310-cp310-macosx_14_0_x86_64.whl (25.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.1/25.1 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, monai\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.1 monai-1.4.0 pillow-11.1.0 pyparsing-3.2.3 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch monai nibabel scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764c4bc",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import glob \n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    Rand3DElasticd,\n",
    "    SpatialPadd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "import time\n",
    "from pytorchtools import EarlyStopping\n",
    "from monai.losses import SSIMLoss as SSIM\n",
    "\n",
    "from monai.utils import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.inferers import sliding_window_inference\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c502ba9",
   "metadata": {},
   "source": [
    "Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5587dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (16, 32)\n",
    "batch_size = (32, 64, 128)\n",
    "lr = (0.0001, 0.001, 0.01, 0.05)\n",
    "filter_num = (16, 32, 64)\n",
    "depth = (3, 4)\n",
    "num_conv = (2, 3)\n",
    "loss_func = \"mae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"output/patch-{patch_size}_batch-{batch_size}_LR-{lr}_filter-{filter_num}_depth-{depth}_convs-{num_conv}_loss-{loss_func}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f09751",
   "metadata": {},
   "source": [
    "Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46579389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available \n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32da08",
   "metadata": {},
   "source": [
    "Move preprocessed data to one input folder \n",
    "TO BE CHANGED BASED ON PATH TO PREPROCESSING OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3634f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root = \"/inputs/rigid/\"\n",
    "subjects = sorted(glob.glob(os.path.join(bids_root, \"sub-*\")))\n",
    "\n",
    "data_dicts = []\n",
    "for sub in subjects:\n",
    "    gad_images = glob.glob(os.path.join(sub, \"anat\", \"*gad.nii.gz\"))\n",
    "    nogad_images = glob.glob(os.path.join(sub, \"anat\", \"*nogad.nii.gz\"))\n",
    "    if gad_images and nogad_images:\n",
    "        data_dicts.append({\"image\": gad_images[0], \"label\": nogad_images[0]})\n",
    "\n",
    "print(\"Loaded\", len(data_dicts), \"paired samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281132ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_tr=data_dicts[0]# training file\n",
    "radius_actual = [int(patch_size/2-1)]*3 # getting c3d patch radius ie. if 32 ^3 patch size, it is 15\n",
    "patch_radius= np.array(radius_actual) # Patch dimensions\n",
    "dims = 1+2*patch_radius # numpyt\n",
    "dims_tuple = (patch_size,)*3\n",
    "k = 2  # Number of channels\n",
    "bps = (4 * k * np.prod(dims)) # Bytes per sample\n",
    "np_tr = os.path.getsize(fname_tr) // bps  # Number of samples\n",
    "arr_shape_tr= (int(np_tr),dims[0],dims[1],dims[2], k)\n",
    "arr_train = np.memmap(fname_tr,'float32','r+',shape=arr_shape_tr)\n",
    "\n",
    "fname_va=data_dicts[1] # validation file   \n",
    "np_va = os.path.getsize(fname_va) // bps      # Number of samples\n",
    "arr_shape_va= (int(np_va),dims[0],dims[1],dims[2], k)\n",
    "arr_val= np.memmap(fname_va,'float32','r+',shape=arr_shape_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699898c6",
   "metadata": {},
   "source": [
    "Split into train, test, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da63673",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(data_dicts, test_size=0.15, random_state=42)\n",
    "\n",
    "# 0.176 ≈ 15% of the full data\n",
    "train, val = train_test_split(train_val, test_size=0.176, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4da5e",
   "metadata": {},
   "source": [
    "Define transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to train with patches\n",
    "train_transforms = Compose([\n",
    "    SpatialPadd(keys = (\"image\",\"label\"), spatial_size = dims_tuple), #ensures all data is around the same size\n",
    "    Rand3DElasticd(keys = (\"image\",\"label\"), sigma_range = (0.5,1), magnitude_range = (0.1, 0.4), prob=0.4, shear_range=(0.1, -0.05, 0.0, 0.0, 0.0, 0.0), scale_range=0.5, padding_mode= \"zeros\"),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=1),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=0),\n",
    "    RandFlipd(keys = (\"image\",\"label\"), prob = 0.5, spatial_axis=2),\n",
    "    RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=patch_size, random_center=True, random_size=False)\n",
    "])\n",
    "\n",
    "# want to validate and test with whole images \n",
    "val_transforms = Compose([\n",
    "    SpatialPadd(keys = (\"image\",\"label\"),spatial_size = dims_tuple)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a34313",
   "metadata": {},
   "source": [
    "Set up datasets and data loader with monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef550ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = Dataset(data=train, transform=train_transforms)\n",
    "val_ds = Dataset(data=val, transform=val_transforms)\n",
    "test_ds = Dataset(data=test, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=pin_memory)\n",
    "\n",
    "# val and test on whole brain data \n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e81649",
   "metadata": {},
   "source": [
    "Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bee68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate channels and strides based on given depth \n",
    "\n",
    "# question about this section - does this match the unet? \n",
    "channels = []\n",
    "for i in range(depth):\n",
    "    channels.append(filter)\n",
    "    filter *=2\n",
    "print(\"channels: \", channels)\n",
    "\n",
    "strides = []\n",
    "for i in range(depth - 1):\n",
    "    strides.append(2)\n",
    "strides += 1\n",
    "print(\"strides: \", strides)\n",
    "\n",
    "# define model \n",
    "model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=channels,\n",
    "    strides=strides,\n",
    "    num_res_units=2,\n",
    "    dropout=0.2,\n",
    "    norm='BATCH'\n",
    ").to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea381b",
   "metadata": {},
   "source": [
    "Define loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = int(np_tr / batch_size) # number of training steps per epoch\n",
    "validation_steps = int(np_va / batch_size)\n",
    "learning_rate = float(lr)\n",
    "betas = (0.5, 0.999)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas=betas)\n",
    "patience = 22 # epochs it will take for training to terminate if no improvement\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, path = f'{output_dir}/checkpoint.pt')\n",
    "max_epochs = 800\n",
    "\n",
    "loss = torch.nn.L1Loss().to(device)\n",
    "\n",
    "train_losses = [float('inf')]\n",
    "val_losses = [float('inf')]\n",
    "test_losses = [float('inf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a74d64",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7898ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar(\n",
    "        index=epoch+1, # displays what step we are of current epoch, our epoch number, training  loss\n",
    "        count = max_epochs, \n",
    "        desc= f\"epoch {epoch + 1}, training mae loss: {train_losses[-1]:.4f}, validation mae metric: {val_losses[-1]:.4f}\",\n",
    "        newline = True) # progress bar to display current stage in training\n",
    "    \n",
    "    # training\n",
    "    for batch in train_loader:\n",
    "        # image is gad image, label is nogad image\n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        degad_images = model(gad_images)\n",
    "        train_loss = loss(degad_images, nogad_images)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_train_loss += train_loss.item() \n",
    "        avg_train_loss /= training_steps\n",
    "        train_losses.append(avg_train_loss) # append total epoch loss divided by the number of training steps in epoch to loss list\n",
    "        model.eval()\n",
    "    \n",
    "    # validation \n",
    "    with torch.no_grad(): #we do not update weights/biases in validation training, only used to assess current state of model\n",
    "        avg_val_loss = 0 # will hold sum of all validation losses in epoch and then average\n",
    "        for batch in val_loader: # iterating through dataloader\n",
    "            gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "            degad_images = model(gad_images)\n",
    "            \n",
    "            val_loss = loss(degad_images, nogad_images)\n",
    "            avg_val_loss += val_loss \n",
    "        avg_val_loss = avg_val_loss.item()/validation_steps #producing average val loss for this epoch\n",
    "        val_losses.append(avg_val_loss) \n",
    "        early_stopping(avg_val_loss, model) # early stopping keeps track of last best model\n",
    "\n",
    "    if early_stopping.early_stop: # stops early if validation loss has not improved for {patience} number of epochs\n",
    "        print(\"Early stopping\") \n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "time = end - start\n",
    "print(\"time for training and validation: \", time)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e97b9b",
   "metadata": {},
   "source": [
    "Plot training metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{output_dir}/model_stats.txt', 'w') as file:  \n",
    "    file.write(f'Training time: {time}\\n') \n",
    "    file.write(f'Number of trainable parameters: {trainable_params}\\n')\n",
    "    file.write(f'Training loss: {train_losses[-patience]} \\nValidation loss: {early_stopping.val_loss_min}')\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(list(range(len(train_losses))), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(list(range(len(val_losses))),val_losses , label=\"Validation Loss\")\n",
    "    plt.grid(True, \"both\", \"both\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{output_dir}/lossfunction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0c76a",
   "metadata": {},
   "source": [
    "Test model with sliding window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{output_dir}/checkpoint.pt'))\n",
    "model.eval()\n",
    "\n",
    "output_dir_test = Path(output_dir) / \"test\"\n",
    "output_dir_test.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        gad_images, nogad_images = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "        degad_images = sliding_window_inference(gad_images, patch_size, 1, model)\n",
    "\n",
    "        loss_val = loss(degad_images, nogad_images)\n",
    "\n",
    "        test_loss += loss_val.item()\n",
    "\n",
    "        # to save the output files \n",
    "        for i in range(degad_images.shape[0]):\n",
    "            gad_path = batch[\"image_meta_dict\"][\"filename_or_obj\"][i]\n",
    "            gad_nib = nib.load(gad_path)\n",
    "            sub = Path(gad_path).name.split(\"_\")[0] \n",
    "            degad_name = f\"{sub}_acq-degad_T1w.nii.gz\"\n",
    "            \n",
    "            degad_nib = nib.Nifti1Image(\n",
    "                degad_images[i, 0].cpu().numpy() * 100, \n",
    "                affine=gad_nib.affine,\n",
    "                header=gad_nib.header\n",
    "            )\n",
    "\n",
    "            save_path = output_dir_test / degad_name\n",
    "            nib.save(degad_nib, str(save_path))\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
